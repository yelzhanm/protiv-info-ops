import json
from pathlib import Path

def validate_and_fix_project_json():
    
    json_file = Path('data/project.json')
    
    if not json_file.exists():
        print("‚ùå –§–∞–π–ª data/project.json –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return
    
    print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ project.json...")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è JSON
    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except json.JSONDecodeError as e:
        print(f"‚ùå –û–®–ò–ë–ö–ê: –ù–µ–≤–µ—Ä–Ω—ã–π JSON —Ñ–æ—Ä–º–∞—Ç!")
        print(f"   –°—Ç—Ä–æ–∫–∞ {e.lineno}, –ø–æ–∑–∏—Ü–∏—è {e.colno}")
        print(f"   {e.msg}")
        return
    
    if not isinstance(data, list):
        print("‚ùå –û–®–ò–ë–ö–ê: project.json –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–∞—Å—Å–∏–≤ –æ–±—ä–µ–∫—Ç–æ–≤")
        return
    
    print(f"‚úÖ JSON —Å–∏–Ω—Ç–∞–∫—Å–∏—Å –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω")
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(data)}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = {
        'total': len(data),
        'valid': 0,
        'missing_text': 0,
        'missing_labels': 0,
        'duplicates': 0,
        'fixed': []
    }
    
    seen_texts = set()
    cleaned_data = []
    
    for idx, item in enumerate(data):
        issues = []
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ (—Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã)
        text = None
        if 'text' in item:
            text = item['text']
        elif 'data' in item and isinstance(item['data'], dict):
            text = item['data'].get('text')
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ç–µ–∫—Å—Ç–∞
        if not text or not text.strip():
            stats['missing_text'] += 1
            issues.append(f"  ‚ö†Ô∏è –ó–∞–ø–∏—Å—å #{idx}: –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç")
            continue
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        if text in seen_texts:
            stats['duplicates'] += 1
            issues.append(f"  ‚ö†Ô∏è –ó–∞–ø–∏—Å—å #{idx}: –î—É–±–ª–∏–∫–∞—Ç —Ç–µ–∫—Å—Ç–∞")
            continue
        
        seen_texts.add(text)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–µ—Ç–æ–∫ (io_type, emo_eval, fake_claim)
        labels_found = False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç–æ–∫
        if 'io_type' in item or 'emo_eval' in item or 'fake_claim' in item:
            labels_found = True
        elif 'annotations' in item and item['annotations']:
            # –§–æ—Ä–º–∞—Ç Label Studio
            for annotation in item['annotations']:
                if 'result' in annotation and annotation['result']:
                    labels_found = True
                    break
        
        if not labels_found:
            stats['missing_labels'] += 1
            issues.append(f"  ‚ö†Ô∏è –ó–∞–ø–∏—Å—å #{idx}: –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –º–µ—Ç–∫–∏ (io_type, emo_eval, fake_claim)")
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã, –Ω–æ —Ç–µ–∫—Å—Ç –≤–∞–ª–∏–¥–Ω—ã–π - –ø—ã—Ç–∞–µ–º—Å—è –∏—Å–ø—Ä–∞–≤–∏—Ç—å
        if issues and text:
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            normalized = {
                'text': text.strip(),
                'source': item.get('source') or item.get('data', {}).get('source', 'Unknown'),
                'date': item.get('date') or item.get('data', {}).get('date', ''),
                'io_type': item.get('io_type'),
                'emo_eval': item.get('emo_eval'),
                'fake_claim': item.get('fake_claim')
            }
            
            # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –º–µ—Ç–∫–∏ –∏–∑ annotations
            if 'annotations' in item and item['annotations']:
                for annotation in item['annotations']:
                    for result in annotation.get('result', []):
                        from_name = result.get('from_name')
                        value = result.get('value', {}).get('choices', [''])[0]
                        
                        if from_name == 'io_type' and not normalized['io_type']:
                            normalized['io_type'] = value
                        elif from_name == 'emo_eval' and not normalized['emo_eval']:
                            normalized['emo_eval'] = value
                        elif from_name == 'fake_claim' and not normalized['fake_claim']:
                            normalized['fake_claim'] = value
            
            cleaned_data.append(normalized)
            stats['fixed'].append(idx)
        
        elif not issues:
            # –ó–∞–ø–∏—Å—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è
            stats['valid'] += 1
            cleaned_data.append(item)
        
        # –í—ã–≤–æ–¥ –ø—Ä–æ–±–ª–µ–º
        for issue in issues:
            print(issue)
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "="*50)
    print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–û–í–ï–†–ö–ò")
    print("="*50)
    print(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π:           {stats['total']}")
    print(f"‚úÖ –ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö:           {stats['valid']}")
    print(f"üîß –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ:           {len(stats['fixed'])}")
    print(f"‚ö†Ô∏è  –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç:         {stats['missing_text']}")
    print(f"‚ö†Ô∏è  –î—É–±–ª–∏–∫–∞—Ç—ã:            {stats['duplicates']}")
    print(f"‚ö†Ô∏è  –ë–µ–∑ –º–µ—Ç–æ–∫:            {stats['missing_labels']}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏
    if cleaned_data and len(cleaned_data) < len(data):
        backup_file = Path('data/project.json.backup')
        
        print(f"\nüíæ –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏: {backup_file}")
        with open(backup_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞...")
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(cleaned_data, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ –§–∞–π–ª –∏—Å–ø—Ä–∞–≤–ª–µ–Ω! –£–¥–∞–ª–µ–Ω–æ {len(data) - len(cleaned_data)} –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π")
    
    elif len(cleaned_data) == len(data):
        print("\n‚úÖ –í—Å–µ –∑–∞–ø–∏—Å–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ! –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –Ω–µ —Ç—Ä–µ–±—É—é—Ç—Å—è.")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print("\n" + "="*50)
    print("üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
    print("="*50)
    
    if stats['missing_text'] > 0:
        print("‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω—ã –∑–∞–ø–∏—Å–∏ —Å –ø—É—Å—Ç—ã–º —Ç–µ–∫—Å—Ç–æ–º - –æ–Ω–∏ –±—É–¥—É—Ç –ø—Ä–æ–ø—É—â–µ–Ω—ã –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ")
    
    if stats['duplicates'] > 0:
        print("‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã - –æ–Ω–∏ –±—ã–ª–∏ —É–¥–∞–ª–µ–Ω—ã –∏–∑ –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞")
    
    if stats['missing_labels'] > 0:
        print("‚ö†Ô∏è  –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–ø–∏—Å–∏ –±–µ–∑ –º–µ—Ç–æ–∫ - –º–æ–¥–µ–ª—å –Ω–µ —Å–º–æ–∂–µ—Ç –æ–±—É—á–∏—Ç—å—Å—è –Ω–∞ –Ω–∏—Ö")
        print("   –†–µ—à–µ–Ω–∏–µ: –î–æ–±–∞–≤—å—Ç–µ –º–µ—Ç–∫–∏ –≤—Ä—É—á–Ω—É—é –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
    
    if cleaned_data:
        print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ! –í–∞–ª–∏–¥–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞: {len(cleaned_data)}")
        print("\nüìå –°–õ–ï–î–£–Æ–©–ò–ô –®–ê–ì:")
        print("   python migrate_to_sqlite.py")


if __name__ == "__main__":
    validate_and_fix_project_json()