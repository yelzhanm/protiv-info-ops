import json
import warnings
import numpy as np
import sqlite3
import os
from rapidfuzz import fuzz
from sentence_transformers import SentenceTransformer
from sklearn.ensemble import IsolationForest
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from transformers import pipeline, logging
from datetime import datetime
from pathlib import Path
import joblib
from dotenv import load_dotenv

load_dotenv()

BASE_DIR = Path(__file__).resolve().parent
THESAURUS_FILE = BASE_DIR / "data" / "thesaurus.json"
DB_PATH = BASE_DIR / "data" / "db.sqlite"

# üÜï –§–ò–ö–°–ò–†–û–í–ê–ù–ù–´–ï –ö–ê–¢–ï–ì–û–†–ò–ò IO_TYPE
VALID_IO_TYPES = [
    "DISINFORMATION",
    "DEMORALIZATION", 
    "DISCREDITATION",
    "INTIMIDATION",
    "HATE_INCITEMENT",
    "PANIC_CREATION",
    "PROVOCATION",
    "AUTHORITY_UNDERSCORE"
]

# üÜï –ü–ï–†–ï–ö–õ–Æ–ß–ï–ù–ò–ï –ú–ï–ñ–î–£ OLLAMA –ò GROQ
USE_GROQ = os.getenv('USE_GROQ', 'true').lower() == 'true'
GROQ_API_KEY = os.getenv('GROQ_API_KEY', '')

# –°—Ç–∞—Ä–∞—è –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ USE_GROQ=false)
OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'llama3')

logging.set_verbosity_error()
warnings.filterwarnings("ignore", category=FutureWarning)

class NLPAnalyzer:
    def __init__(self):
        print("--- NLP-–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è–ª–∞–Ω–¥—ã ---")
        self.thesaurus = self._load_thesaurus()
        self.io_classifier = None
        self.vectorizer = None
        self.anomaly_model = None
        self._load_hf_models()
        self.model_path = BASE_DIR / "data" / "models.pkl"
        
        # üÜï –ü—Ä–æ–≤–µ—Ä–∫–∞ LLM –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
        if USE_GROQ:
            if not GROQ_API_KEY:
                print("‚ö†Ô∏è GROQ_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env. LLM –∞–Ω–∞–ª–∏–∑ –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")
                print("–ü–æ–ª—É—á–∏—Ç–µ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –∫–ª—é—á –Ω–∞: https://console.groq.com/keys")
            else:
                print("‚úÖ Groq API –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
        else:
            print("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Ollama (—Ç—Ä–µ–±—É–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞)")

    def _load_thesaurus(self):
        try:
            with open(THESAURUS_FILE, "r", encoding="utf-8") as f:
                print(f"‚úÖ –¢–µ–∑–∞—É—Ä—É—Å '{THESAURUS_FILE}' —Å”ô—Ç—Ç—ñ –∂“Ø–∫—Ç–µ–ª–¥—ñ.")
                return json.load(f)
        except FileNotFoundError:
            print(f"‚ùå “ö–ê–¢–ï: –¢–µ–∑–∞—É—Ä—É—Å —Ñ–∞–π–ª—ã '{THESAURUS_FILE}' —Ç–∞–±—ã–ª–º–∞–¥—ã.")
            return []

    def _load_hf_models(self):
        print("üîÑ Hugging Face –º–æ–¥–µ–ª—å–¥–µ—Ä—ñ –∂“Ø–∫—Ç–µ–ª—É–¥–µ...")
        self.ner_model = pipeline(
            "ner", 
            model="Babelscape/wikineural-multilingual-ner", 
            aggregation_strategy="simple"
        )
        self.sentiment_model = pipeline(
            "text-classification", 
            model="blanchefort/rubert-base-cased-sentiment"
        )
        self.embedder = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        print("‚úÖ Hugging Face –º–æ–¥–µ–ª—å–¥–µ—Ä—ñ –∂“Ø–∫—Ç–µ–ª–¥—ñ.")

    def train_models_from_db(self, db_path=str(DB_PATH)):
        """–ú–æ–¥–µ–ª—å–¥–µ—Ä–¥—ñ SQLite –±–∞–∑–∞—Å—ã–Ω–∞–Ω –æ“õ—ã—Ç—É"""
        if self.model_path.exists():
            print("üì• –°–∞“õ—Ç–∞–ª“ì–∞–Ω –º–æ–¥–µ–ª—å–¥–µ—Ä –∂“Ø–∫—Ç–µ–ª—É–¥–µ...")
            try:
                models = joblib.load(self.model_path)
                self.vectorizer = models['vectorizer']
                self.io_classifier = models['classifier']
                self.anomaly_model = models['anomaly']
                print("‚úÖ –ú–æ–¥–µ–ª—å–¥–µ—Ä –∂“Ø–∫—Ç–µ–ª–¥—ñ!")
                return
            except Exception as e:
                print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å–¥—ñ –∂“Ø–∫—Ç–µ—É “õ–∞—Ç–µ—Å—ñ: {e}, “õ–∞–π—Ç–∞ –æ“õ—ã—Ç—É –±–∞—Å—Ç–∞–ª–∞–¥—ã...")

        print("\n--- –ú–æ–¥–µ–ª—å–¥–µ—Ä–¥—ñ –±–∞–∑–∞–¥–∞–Ω “Ø–π—Ä–µ—Ç—É –±–∞—Å—Ç–∞–ª–¥—ã ---")
        
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT text, io_type FROM messages WHERE io_type IS NOT NULL AND text IS NOT NULL")
            rows = cursor.fetchall()
            conn.close()
        except Exception as e:
            print(f"‚ùå –ë–∞–∑–∞–¥–∞–Ω –æ“õ—É “õ–∞—Ç–µ—Å—ñ: {e}")
            return

        if not rows:
            print("‚ö†Ô∏è –ï–°–ö–ï–†–¢–£: –ë–∞–∑–∞ –±–æ—Å –Ω–µ–º–µ—Å–µ –¥–µ—Ä–µ–∫—Ç–µ—Ä –∂–æ“õ.")
            return

        texts = [r[0] for r in rows]
        labels = [r[1] for r in rows]
        
        # üÜï –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –ú–ï–¢–û–ö –ö –í–ê–õ–ò–î–ù–´–ú –ö–ê–¢–ï–ì–û–†–ò–Ø–ú
        normalized_labels = []
        for label in labels:
            # –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –ø–æ—Ö–æ–∂—É—é –≤–∞–ª–∏–¥–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            label_upper = label.upper()
            if label_upper in VALID_IO_TYPES:
                normalized_labels.append(label_upper)
            else:
                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ, –±–µ—Ä–µ–º DISINFORMATION –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                print(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–µ—Ç–∫–∞ '{label}' –∑–∞–º–µ–Ω–µ–Ω–∞ –Ω–∞ DISINFORMATION")
                normalized_labels.append("DISINFORMATION")
        
        labels = normalized_labels

        unique_labels = set(labels)
        if len(unique_labels) >= 2:
            self.vectorizer = TfidfVectorizer(max_features=1500, ngram_range=(1,2))
            X = self.vectorizer.fit_transform(texts)
            self.io_classifier = LogisticRegression(max_iter=1000, class_weight='balanced')
            self.io_classifier.fit(X, labels)
            print(f"‚úÖ –ê–û –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–∞–π—ã–Ω ({len(rows)} –∂–∞–∑–±–∞).")
        else:
            print(f"‚ö†Ô∏è –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä “Ø–π—Ä–µ—Ç—ñ–ª–º–µ–¥—ñ. –ë—ñ—Ä “ì–∞–Ω–∞ –∫–ª–∞—Å—Å –±–∞—Ä: {unique_labels}")

        self.anomaly_model = IsolationForest(contamination=0.1, random_state=42)
        self.anomaly_model.fit(self.embedder.encode(texts))
        print("‚úÖ –ê–Ω–æ–º–∞–ª–∏—è –º–æ–¥–µ–ª—ñ –¥–∞–π—ã–Ω.")

        if self.io_classifier:
            print("üíæ –ú–æ–¥–µ–ª—å–¥–µ—Ä —Å–∞“õ—Ç–∞–ª—É–¥–∞...")
            joblib.dump({
                'vectorizer': self.vectorizer,
                'classifier': self.io_classifier,
                'anomaly': self.anomaly_model
            }, self.model_path)

    def analyze_single_message(self, message_object):
        text = message_object.get("text", "")
        if not text: return None

        ner_results = self.ner_model(text)
        sentiment_result = self.sentiment_model(text)[0]

        custom_rules = {"–†–§": "ORG", "–†–æ—Å—Å–∏–∏": "LOC", "–í–°–£": "ORG", "–£–∫—Ä–∞–∏–Ω–µ": "LOC", "–°–®–ê": "LOC"}
        ner_entities = [{"entity": e.get("entity_group", "UNKNOWN"), "word": e.get("word")} for e in ner_results]
        for entity in ner_entities:
            if entity["word"] in custom_rules:
                entity["entity"] = custom_rules[entity["word"]]

        thesaurus_matches = self._find_thesaurus_terms(text)

        io_prediction = "DISINFORMATION"  # üÜï –î–ï–§–û–õ–¢–ù–û–ï –ó–ù–ê–ß–ï–ù–ò–ï
        if self.io_classifier and self.vectorizer:
            try:
                predicted = self.io_classifier.predict(self.vectorizer.transform([text]))[0]
                # üÜï –ü–†–û–í–ï–†–ö–ê –ù–ê –í–ê–õ–ò–î–ù–û–°–¢–¨
                if predicted.upper() in VALID_IO_TYPES:
                    io_prediction = predicted.upper()
                else:
                    print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–ª–∞ –Ω–µ–≤–∞–ª–∏–¥–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é: {predicted}")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")

        is_anomaly = False
        if self.anomaly_model:
            is_anomaly = True if self.anomaly_model.predict(self.embedder.encode([text]))[0] == -1 else False
        
        # üÜï –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï GROQ –ò–õ–ò OLLAMA
        llm_analysis = self._get_llm_summary(text, ner_entities, thesaurus_matches)

        return {
            "source_info": {"channel": message_object.get("channel"), "date": message_object.get("date")},
            "original_text": text,
            "analysis_report": {
                "predicted_info_operation_type": io_prediction,
                "is_anomaly": is_anomaly,
                "general_sentiment": {"label": sentiment_result['label'], "score": round(sentiment_result['score'], 3)},
                "military_terms_analysis": thesaurus_matches,
                "named_entities_recognition": ner_entities,
                "llm_expert_summary": llm_analysis
            }
        }

    def _find_thesaurus_terms(self, text, threshold=85):
        matches = []
        if not self.thesaurus: return matches
        term_types = {
            "–ù–µ–≥—ñ–∑–≥—ñ —Ç–µ—Ä–º–∏–Ω": ["TT_kz", "TT_ru", "TT_en"],
            "–°–∏–Ω–æ–Ω–∏–º": ["UF_kz", "UF_ru", "UF_en"],
            "–ë–∞–π–ª–∞–Ω—ã—Å—Ç—ã —Ç–µ—Ä–º–∏–Ω": ["RT_kz", "RT_ru", "RT_en"]
        }
        for term in self.thesaurus:
            found = False
            for type_name, keys in term_types.items():
                for key in keys:
                    alias = term.get(key)
                    if alias and fuzz.partial_ratio(alias.lower(), text.lower()) > threshold:
                        matches.append({
                            "id": term.get("id"),
                            "term_kz": term.get("TT_kz"),
                            "term_ru": term.get("TT_ru"),
                            "term_en": term.get("TT_en"),
                            "matched_alias": alias,
                            "match_type": type_name
                        })
                        found = True
                        break
                if found: break
        return matches

    def _get_llm_summary(self, text, ner, thesaurus):
        """
        üÜï –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –î–õ–Ø LLM –ê–ù–ê–õ–ò–ó–ê
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –º–µ–∂–¥—É Groq –∏ Ollama
        """
        
        prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:

–¢–µ–∫—Å—Ç: "{text}"

–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏: {ner}
–í–æ–µ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã: {thesaurus}

–ó–∞–¥–∞—á–∞:
1. –ù–∞–ø–∏—à–∏ –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) –æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞
2. –û—Ü–µ–Ω–∏ —É—Ä–æ–≤–µ–Ω—å —É–≥—Ä–æ–∑—ã –æ—Ç 1 –¥–æ 5:
   - 1-2: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π/–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π
   - 3: –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –º–∞–Ω–∏–ø—É–ª—è—Ç–∏–≤–Ω—ã–π
   - 4-5: –Ø–≤–Ω–∞—è –¥–µ–∑–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è/—É–≥—Ä–æ–∑–∞

–û—Ç–≤–µ—Ç—å –°–¢–†–û–ì–û –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ:
{{
  "summary": "–∫—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞",
  "threat_level": —á–∏—Å–ª–æ –æ—Ç 1 –¥–æ 5
}}"""

        if USE_GROQ:
            return self._call_groq_api(prompt)
        else:
            return self._call_ollama_api(prompt)

    def _call_groq_api(self, prompt):
        """
        üÜï –í–´–ó–û–í GROQ API
        –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: https://console.groq.com/docs/quickstart
        """
        if not GROQ_API_KEY:
            return {
                "summary": "LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: GROQ_API_KEY –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω",
                "threat_level": -1
            }
        
        try:
            import requests
            
            response = requests.post(
                'https://api.groq.com/openai/v1/chat/completions',
                headers={
                    'Authorization': f'Bearer {GROQ_API_KEY}',
                    'Content-Type': 'application/json'
                },
                json={
                    'model': 'llama-3.3-70b-versatile',  # –ë–µ—Å–ø–ª–∞—Ç–Ω–∞—è –º–æ–¥–µ–ª—å
                    'messages': [
                        {
                            'role': 'system',
                            'content': '–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ.'
                        },
                        {
                            'role': 'user',
                            'content': prompt
                        }
                    ],
                    'temperature': 0.3,
                    'max_tokens': 500
                },
                timeout=15
            )
            
            if response.status_code != 200:
                print(f"‚ö†Ô∏è Groq API –æ—à–∏–±–∫–∞: {response.status_code}")
                return {
                    "summary": f"–û—à–∏–±–∫–∞ API: {response.status_code}",
                    "threat_level": -1
                }
            
            result = response.json()
            content = result['choices'][0]['message']['content']
            
            # –û—á–∏—Å—Ç–∫–∞ –æ—Ç markdown
            content = content.strip()
            if content.startswith('```json'):
                content = content[7:]
            if content.startswith('```'):
                content = content[3:]
            if content.endswith('```'):
                content = content[:-3]
            content = content.strip()
            
            return json.loads(content)
            
        except requests.Timeout:
            return {
                "summary": "–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è API",
                "threat_level": -1
            }
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
            return {
                "summary": "–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–≤–µ—Ç–∞ LLM",
                "threat_level": -1
            }
        except Exception as e:
            print(f"‚ö†Ô∏è Groq API –æ—à–∏–±–∫–∞: {e}")
            return {
                "summary": f"–û—à–∏–±–∫–∞ LLM: {str(e)}",
                "threat_level": -1
            }

    def _call_ollama_api(self, prompt):
        """
        –°–¢–ê–†–´–ô –ú–ï–¢–û–î –° OLLAMA (–¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏)
        """
        try:
            import ollama
            
            response = ollama.chat(
                model=OLLAMA_MODEL,
                messages=[{'role': 'user', 'content': prompt}],
                format='json',
                options={'timeout': 30}
            )
            
            return json.loads(response['message']['content'])
        except ImportError:
            return {
                "summary": "Ollama –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ USE_GROQ=true",
                "threat_level": -1
            }
        except Exception as e:
            print(f"‚ö† Ollama “õ–∞—Ç–µ—Å—ñ: {e}")
            return {
                "summary": f"–û—à–∏–±–∫–∞ Ollama: {str(e)}",
                "threat_level": -1
            }

if __name__ == "__main__":
    analyzer = NLPAnalyzer()
    analyzer.train_models_from_db()